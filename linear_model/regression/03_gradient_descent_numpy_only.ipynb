{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e36c0b1e",
   "metadata": {},
   "source": [
    "# Gradient Descent with numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f3d639bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fb01f5f",
   "metadata": {},
   "source": [
    "## 1) Intuitive Explanation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81df0b8e",
   "metadata": {},
   "source": [
    "Gradient Descent is an iterative optimization algorithm used to find the minimum of a function. It follows these steps:\n",
    "\n",
    "1. Initialize parameters (often randomly).\n",
    "2. Compute the gradient (the vector of first-order derivatives) to see how the function is changing locally.\n",
    "3. Update parameters by moving in the opposite direction of the gradient, scaled by a learning rate (Î±). The learning rate controls both the convergence speed and the ability to actually reach the minimum.\n",
    "4. Repeat this process until convergence, i.e., until the parameter updates become very small or a maximum number of iterations is reached.\n",
    "\n",
    "Because the Mean Squared Error (MSE) is a convex function, gradient descent is guaranteed to converge to the global minimum (if the learning rate is well chosen)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a68b193",
   "metadata": {},
   "source": [
    "## 2) Mathematical Formulation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba229908",
   "metadata": {},
   "source": [
    "We consider a simple linear regression model:  \n",
    "\n",
    "$$\n",
    "\\hat{y}^{(i)} = w x^{(i)} + b\n",
    "$$  \n",
    "\n",
    "\n",
    "\n",
    "**Cost function (Mean Squared Error, MSE):**  \n",
    "\n",
    "$$\n",
    "J(w, b) = \\frac{1}{m} \\sum_{i=1}^{m} \\left( \\hat{y}^{(i)} - y^{(i)} \\right)^2\n",
    "$$  \n",
    "\n",
    "where:  \n",
    "- $m$ = number of training samples  \n",
    "- $y^{(i)}$ = true value  \n",
    "- $\\hat{y}^{(i)}$ = predicted value  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f85ba51c",
   "metadata": {},
   "source": [
    "**Gradients:**  \n",
    "\n",
    "$$\n",
    "\\frac{\\partial J}{\\partial w} = \\frac{2}{m} \\sum_{i=1}^{m} \\left( (w x^{(i)} + b) - y^{(i)} \\right) x^{(i)}\n",
    "$$  \n",
    "\n",
    "$$\n",
    "\\frac{\\partial J}{\\partial b} = \\frac{2}{m} \\sum_{i=1}^{m} \\left( (w x^{(i)} + b) - y^{(i)} \\right)\n",
    "$$  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9de64d29",
   "metadata": {},
   "source": [
    "\n",
    "**Update rules:**  \n",
    "\n",
    "$$\n",
    "w := w - \\alpha \\cdot \\frac{\\partial J}{\\partial w}\n",
    "$$  \n",
    "\n",
    "$$\n",
    "b := b - \\alpha \\cdot \\frac{\\partial J}{\\partial b}\n",
    "$$  \n",
    "\n",
    "where $\\alpha$ is the learning rate.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "372436b2",
   "metadata": {},
   "source": [
    "## Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82bfe486",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "linear-model-py3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
